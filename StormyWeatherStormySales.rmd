---
title: "Stormy Weather, Stormy Sales"
author: "Jonathan Lu and Kara Wong"
date: "12/13/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

```{r load-packages, include = FALSE}
library(lubridate)
library(hydroTSM)
library(imputeTS)
library(rlist)
library(tidyverse)
library(caret)
library(knitr)
library(kableExtra)
library(ggplot2)
library(gridExtra)
library(egg)
library(corrplot)
library(glmnet)
library(MASS)
library(car)
```

```{r load-data}
trn_c = read_csv("data/trn_c.csv")
```

```{r load-s-data}
trn_s = read_csv("data/trn_s.csv")
```

```{r load-test}
tst = read_csv("data/tst_c.csv")
```


```{r transform-variables}
trn_c$isWeekend = as.factor(trn_c$isWeekend)
trn_c$item_nbr = as.factor(trn_c$item_nbr)
trn_c$codesum = as.factor(trn_c$codesum)
trn_c$month = substring(trn_c$date, 6, 7)
```

```{r converting_factors_trn}
trn_s$store_nbr = as.factor(trn_s$store_nbr)
trn_s$item_nbr = as.factor(trn_s$item_nbr)
trn_s$station_nbr = as.factor(trn_s$station_nbr)
trn_s$isWeekend = as.factor(ifelse(trn_s$isWeekend, "weekend", "weekday"))
levels(trn_s$isWeekend) = c("weekday", "weekend")
trn_s$store_nbr = as.factor(trn_s$store_nbr)
trn_s$codesum = as.factor(ifelse(trn_s$codesum, "event", "no-event"))
levels(trn_s$codesum) = c("no-event", "event")
trn_s$month = substring(trn_s$date, 6, 7)
```

```{r converting_factors_tst}
tst$store_nbr = as.factor(tst$store_nbr)
tst$item_nbr = as.factor(tst$item_nbr)
tst$station_nbr = as.factor(tst$station_nbr)
tst$isWeekend = as.factor(ifelse(tst$isWeekend, "weekend", "weekday"))
levels(tst$isWeekend) = c("weekday", "weekend")
tst$store_nbr = as.factor(tst$store_nbr)
tst$codesum = as.factor(ifelse(tst$codesum, "event", "no-event"))
levels(tst$codesum) = c("no-event", "event")
```

```{r EDA-top5-items}
# categorize to top 5 selling items, and all else
trn_c_nbr = trn_c %>% 
  group_by(item_nbr) %>% 
  summarise(nbr_unit = sum(units))

item_rank = trn_c_nbr %>% 
  arrange(desc(nbr_unit))

top5_items = item_rank$item_nbr[1:5]

trn_c_top5 = trn_c %>% 
  filter(item_nbr %in% top5_items)

trn_c_cat = trn_c %>% 
  mutate("top5_nbr" = ifelse(trn_c$item_nbr %in% top5_items, trn_c$item_nbr, "Other"))
```

```{r EDA-trn-c-plots, cache = TRUE}
# we can see that most items in our data have low number of sales:
c01 = trn_c_nbr %>%
  ggplot(aes(x = nbr_unit)) +
  ggtitle("# Units Sold per Item") +
  geom_histogram(bins = 30) +
    xlab("units sold per item type")

c02 = trn_c_nbr %>%
  filter(nbr_unit < 2000) %>% 
  ggplot(aes(x = nbr_unit)) +
  geom_histogram(bins = 30) +
  theme(legend.position = "none") +
  ggtitle("Items with Units Sold < 2000") +
  xlab("units sold per item type")

c03 = trn_c_nbr %>%
  filter(nbr_unit > 3000) %>% 
  ggplot(aes(x = nbr_unit)) +
  geom_histogram() + 
  theme(legend.position = "none") +
  ggtitle("Items with Units Sold > 3000") +
  xlab("units sold per item type")

# c04 - c015: number of sales based on weather-driven factors
# if not a top5-selling item, we grouped it into the "Other" bucket
c04 = trn_c_cat %>% 
  filter(tmax > 10) %>% 
  ggplot(aes(x = tmax, weights = units, fill = top5_nbr)) +
  geom_histogram()

c05 = trn_c_cat %>%
  filter(tmin > -5) %>% 
  ggplot(aes(x = tmin, weights = units, fill = top5_nbr)) +
  geom_histogram()


c06 = trn_c_cat %>% 
  ggplot(aes(x = dewpoint, weights = units, fill = top5_nbr)) +
  geom_histogram()

c07 = trn_c_cat %>% 
  ggplot(aes(x = stnpressure, weights = units, fill = top5_nbr)) +
  geom_histogram()

c08 = trn_c_cat %>% 
  ggplot(aes(x = wetbulb, weights = units, fill = top5_nbr)) +
  geom_histogram()

c09 = trn_c_cat %>% 
  ggplot(aes(x = resultspeed, weights = units, fill = top5_nbr)) +
  geom_histogram()

c10 = trn_c_cat %>% 
  ggplot(aes(x = heat, weights = units, fill = top5_nbr)) +
  geom_histogram()

c11 = trn_c_cat %>% 
  ggplot(aes(x = resultdir, weights = units, fill = top5_nbr)) +
  geom_histogram()

c12 = trn_c_cat %>% 
  ggplot(aes(x = cool, weights = units, fill = top5_nbr)) +
  geom_histogram()

c13 = trn_c_cat %>% 
  ggplot(aes(x = avgspeed, weights = units, fill = top5_nbr)) +
  geom_histogram()

c14 = trn_c_cat %>% 
  ggplot(aes(x = as.factor(codesum), weights = units, fill = top5_nbr)) +
  geom_bar() +
  xlab("weather event")

c15 = trn_c_cat %>% 
    mutate(day = weekdays(as.Date(date))) %>%
  ggplot(aes(x = day, weights = units, fill = top5_nbr)) +
  geom_bar() + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    scale_x_discrete(limits=c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

# seasonality?

c16 = trn_c_cat %>% 
  ggplot(aes(x = month, weights = units, fill = top5_nbr)) +
  geom_bar()

# temp across stations
c17 = trn_c_cat %>% 
  group_by(station_nbr) %>% 
  summarise(avgtmax = mean(tmax)) %>% 
  ggplot(aes(x = as.factor(station_nbr), y = avgtmax)) +
  geom_col() +
  xlab("Station")

c18 = trn_c_cat %>% 
  group_by(station_nbr) %>% 
  summarise(avgtmin = mean(tmin)) %>% 
  ggplot(aes(x = as.factor(station_nbr), y = avgtmin)) +
  geom_col() +
  xlab("Station")

c19 = trn_c_cat %>% 
  group_by(store_nbr) %>% 
  summarise(totUnits = sum(units)) %>%
  ggplot(aes(x = as.factor(store_nbr), y = totUnits)) +
  geom_col() +
  xlab("Store Number") + 
    ylab("Total Units Sold")

```

```{r writeup-values}
itm_lowUnits1 = trn_c_nbr %>%
  filter(nbr_unit < 500)

itm_lowUnits2 = trn_c_nbr %>%
  filter(nbr_unit < 1000)

lowUnits1_prop = nrow(itm_lowUnits1)/nrow(trn_c_nbr)
lowUnits2_prop = nrow(itm_lowUnits2)/nrow(trn_c_nbr)
```

## Introduction

While we can't control the weather, we can control how we react to it.

Based on a survey conducted by a mobile testing firm, SOASTA, checking the weather is the first thing 45% of participants do in the morning [^1]. The weather is the second most commonly first-checked item in the morning, after checking emails. Weather plays an integral part of society, as it affects our daily lives. It determines how we dress, commute, eat, and feel. For example, if we see that a rainstorm is approaching, we would most likely choose to stay indoors rather than going out to run errands. This extreme influence that weather has on us is reflected in the changes in the U.S. gdp numbers. According to the Atomospheric and Environmental Research Center, thirty percent of U.S. gross domestic product is affected by weather one way or the other [^2]. 

As part of an existing drive to be able to predict consumer behaviors, weather is just another factor which retailers want to consider. There are plenty of advantages to analyzing the effect weather has on a business. One of the largest issues which businesses face today are largely inventory based as floor space is expensive. By anticipating and reacting to weather-driven demand, businesses would be better able to maintian necessary stock during times of need, while other times, they would be able to use the space in the store for other, more profitable items. While this knowledge would be useful across all stores in the US, this would have a far larger effect in stores in metropolitan areas as the floor space is worth a lot more per square foot. Being able to anticipate trends in sales would also allow businesses to work around logistic issues such as transportation of goods. While this benefit ties in to the main one of understanding the necessary stock, optimizing logistic issues surounding transportation of goods between locations will also help businesses lower their operating costs and overhead.

While the idea behind an analysis such as this seem to be extremely useful, actual applications of such a model are extremely limited. When considering models for this data, there is a clear trade of between interpretability and accuracy, the less interpretable, the more accurate. Since this analysis is focused mostly on linear models, the interpretability of the coefficients the most useful aspect to this analysis. This is because while predicting how much you should've stocked is relatively useful information, using models such as the ones explored below to predict how much you should stock will be relatively fruitless. This is because in order to "predict" the future using these models, you will need to have access to accurate weather data in the future. Usually weather data is relatively accurate for one-week out forecasts [^4], however, through experience, everyone knows that the weather forecast can change from day to day. This level of uncertainty leads to uncertainties in the model, which in turn, makes them less reliable. 

The data used in this analysis was provided by Walmart as part of a recruiting event on Kaggle [^3]. On Kaggle, they provided training and testing datasets. In addition to those files, they also provided a key to map stores to stations and a collection of relevant weather station data from the NOAA. The data spans 45 different Walmart locations, 20 different weather stations, and over 2 years of data. The data initially provided is relatively messy as the weather data has a lot of missing values The objective of this analysis is to predict the amount of units sold for each product with regards to major weather events. 

## Exploratory Data Analysis

Our first observation when looking into our dataset is that our dataset has a lot of items with low sales. In fact, out of the 111 items, `r round(lowUnits2_prop*100, 0)`%  have less than 1,000 sales and `r round(lowUnits1_prop*100, 0)`% have less than 500 sales. On the other hand, the top-five highest selling items all have over 10,000 units sold. Below, we have provided summary statistics for the number of units sold per item type, as well as a graphical representation.

```{r quartile-units-p-item, cache = TRUE}
nbrunit_sum = summary(trn_c_nbr$nbr_unit)

tibble(
  "Minimum" = nbrunit_sum[1],
  "First Quartile" = nbrunit_sum[2],
  "Median" = nbrunit_sum[3],
  "Mean" = round(nbrunit_sum[4],0),
  "Third Quartile" = nbrunit_sum[5],
  "Maximum" = nbrunit_sum[6]
  ) %>% 
  kable(digits = 4,
        caption = "Table: Summary of Number of Units Sold per Item Type") %>% 
  kable_styling(latex_options = c("striped", "hold_position"), full_width = FALSE)
```

```{r dist-units-sold, message = FALSE, echo=FALSE, fig.width=8, fig.height=4}
gridExtra::grid.arrange(c01, c02, ncol = 2)
```

From the graphs above, we can see how drastic the difference of sales between items truly is. This discrepancy was briefly touched on in the inital Kaggle posting when Walmart said that some of these items might be everyday objects such as milk. In order to get a better idea on the actual distribution of the number of items sold, if we look at the distributions of items with under 2,000 sold, we can see that most of the items sold are between 500 and 700 units sold. This is relatively interesting as this averages to a little bit under one unit sold a day. The most important thing to get from these graphs however, is still the discrepance between items. Before even fitting models, it's clear that there are going to be many influential "outliers" because the item that sold the most sold 1005111 items which is approximately 1400 units per day across all stores. When compared to the 1 unit per day of most observations, it becomes clear that modeling all of the variables at once is going to be challenging.

An important thing to note about our dataset is that it does not capture the discrepancies between inventory and demand. The number of units sold may show 0, but this may not necessarily mean that the item was not in demand, as it could just simply have been out of stock or discontinued. This may be an issue when it comes to making predictions.

We then decided to analyze the amount of units sold of these top-five selling items based on the weather-based metrics provided (maximum/min temperature, dewpoint, etc.). The five highest selling items from highest to lowest are items: 45, 9, 5, 44, and 16. From the following plots, we can see that more purchases are made when it's hot and humid, when it's not too windy, and on weekdays. The most important takeaway from these plots however, is that the top five selling items all seem to follow a similar distribution when it comes to units sold. This means that if we could hypothetically accurately predict items sold from the weather data, we should be able to accurately predict units sold across most of the items. From the rightmost plot, we can see that the same idea of the weather data holds as well for days in that the items follow a similar distribution across the days of the week. This graph also confirms our initial ideas that the weekend has more sales that the other days of the week, thus leading us to create a categorical variable to represent weekend vs weekday. From the plots below, we can also see that there isn't one item type that is driving sales based on these weather metrics. More of these plots can be found in the Appendix.

```{r 4-plots, message = FALSE, echo = FALSE, cache = TRUE}
gridExtra::grid.arrange(c04, c06, c13, c15, ncol = 2)
```

In addition to comparing the amount of sales on weekdays versus weekends, we also looked to see if there were any seasonal differences that could be driving sales. We extracted the month from `date` and provided a graph below that shows how there seems to be a downward trend in sales starting from January. This is a bit suprising considering that during the winter months, the temperature is approximately the same. While the graph below hint that seasons and months might not be a strong predictor of units sold, we still believe that months might be relevant as we wanted to create a variable that might capture the data represented by temperature and time since we are planning to removing date. Season on the other hand, seems to be completely irrelevant as their isn't a hint of consistency within each season as shown below.

```{r units-by-month, fig.width=7, fig.height=4}
c16
```


From the correlation plot below, we can see that all of the temperature data is highly correlated with each other. From this, we can anticipate that if we ran variable selection on this dataset, we would end up removing multiple variables. The rain and wind data seems more promising as they are very loosely correlated with all the other variables in this data set. While the correlation between the max temperature and the min temperature in a day is expected, we'd hoped for there to be more of a deviation. However, we still decided to remove the average temperature instead of the minimum and maximum temperature as we think that the more extreme the temperature might be, the better a predictor it will be.

```{r EDA-correlation-matrix, fig.width=3, fig.height=3}
cm_data = cor(trn_c[,-c(1,2,3,5,12,18,19,20)])
corrplot(cm_data, method = "circle")
```

Before we could build our first linear model, we had to stitch the weather data to the original dataset provided by Walmart. The main bulk of the work when it came to this process was cleaning the weather data to a point where it could be added together.

At an initial glance, we could see that the weather data was extremely incomplete *Fig.1*. When aggregated to a station level, every single station had some sort of missing data. The variables that were missing the most data were depart, sunrise, sunset, and snowfall. Each of these variables had over 50% of their total data across all stations missing. With this many missing observations, we decided that it would be extremely unreasonable to try and impute data for that many missing observations as we were neither knowledgeable enough about weather, nor knowledgeable enough about the stations themselves to be able to classify these missing observations as anything other than "structurally missing". Therefore, since there was nothing we could do on our own to make these variables complete, we chose to remove them from the data. A variable which was close to the cusp in terms of being "un-imputable" was sea level. However, we quickly noted that sea level was pretty much identical to stnpressure (station pressure) as they both measured approximately the same thing. Since these two variables are pretty much perfectly collinear, we chose to remove sea level as there were far more observations for station pressure. Another variable we removed due to multicollinearity is tavg, or the average temperature of the day. When we looked into it, the value is calculated as the average of the minimum temperature and the maximum temperature. We chose to remove the average because we believed that the more extreme weather experienced, which would be captured better by min and max temperature, the more likely people are to buy weather related gear.

While removing a couple of variables definitely improved on the amount of missing data remaining, almost every single variable and station combination had some number of missing data. However, the amount of data missing at this point was extremely trivial ranging from ~ `r 5/10`% of each station's data. Given that very few observations were missing, we felt relatively comfortable imputing a four day moving average for the missing observations. While this method cleaned up most of the variables for all of the stations except one. Once we aggregated the data to a station level, it became clear that station five had an un-imputable amount of data. Usually in these circumstances, we would remove the station completely, however, since we needed to have matching data for every store in Walmart's initial dataset we had to come up with a way to impute multiple values. To do this, we aggregated the data to a day level and imputed the mean of all the other station's observations at that day. 

Before we finish with our weather data, we decided to create additional variables off of the weather data. When looking over the variables provided, one really stood out and that was date. We thought that it would be incredibly naive to think that the actual value of the date would have some predictive power over the number of units sold so instead; we constructed two categorical variables to suplant the use of the date. We decided that whether or not the day was a weekend and what season the day would fall under would be far more predictive as it narrows down the date to certain features related to it. While we thought that seasons might be really interesting as it sort of has to do with the weather, we were a bit worried about whether or not the change in seasons would already be reflected in the temperature data. However, through our EDA process, it became very clear that there wasn't a clear trend represented by the different seasons, thus leading us to remove it from the first model. We also changed codeSum from a set of strings representing weather events to a categorical variable representing whether or not some weather event happened on the day.

\newpage

## Linear Regression Model

Joining the weather data to the Walmart data was extremely straightforward as all it required was two left joins. The first was to match a station to each store using the provided key. The last join was on the station number and the date, thus giving us a fully merged data set.

When it comes to building the first model, we wanted to keep in as many variables as we could, however, we decided to remove date and station number from model. We chose to remove date because we believed that the actual date would have little to no impact on whether or not someone buys something and instead chose to use our created variables. We also chose to remove station number because the dummy variable that are created to represent each station is perfectly collinear with a certain set of store numbers that the station maps to. The code for the model can be found below.

```{r first_model, include = TRUE}
md1 = lm(units ~ . - date - station_nbr - season, data = trn_s)
md1_coef = summary(md1)$coefficients[3:7,]
```

```{r first_model_kaggle, eval = FALSE}
ss = read_csv("data/sampleSubmission.csv")
f = predict(md1, tst)
f = round(f, 0)
f[f < 0] = 0
ss$units = f
write_csv(ss, "data/first_trial.csv")
write_csv(tst_dat, "data/test_c.csv")
```

Since in this model, we treated store number and item number as a categorical variable, there are `r length(md1$coefficients) - 1` total variables not including the intercept in this model. When looking at the summary output for this model, it becomes clear that matching store numbers is one of the best ways to predict the amount of unit sales. However, like all categorical variables with an extreme number of factors, not all of the levels are significant. Item number also follows a similar trend in that there are some levels which have coefficients indistinguishable from zero, but most of the levels are irrelevant. We think that this is because most of the items sell less than one a day. Both of these issues will be explored further in the next section of this report.

The initial Kaggle score for the initial model with all of the variables is *.74047* which was good for 475th on the leaderboard. 

Taking a deeper dive into the summary of all the stores, there were `r nrow(md1_coef)` stores that had a siginificant coefficient. This is particularly interesting because when looking at the coefficients for each of these stores, all of them were overwhelmingly positive, this means that those stores sold far more than the average number of items when compare to other stores. The positive nature of these coefficients also makes sense when the intercept is taken into consideration. The intercept for this linear model was 1.622 which is pretty interesting because it was insignificant while having a decently large "coefficient". The intercept in this scenario is confounded with the first store and first item which makes its coefficient almost impossible to understand on its own.

```{r LR-pred-v-act-1}
sam = sample(1:nrow(trn_c), round(nrow(trn_c) * .4, 0))
plot(predict(md1)[sam], trn_s$units[sam], xlab = "Fitted", ylab = "Actual")
abline(0,1)
```

When looking at the plot of actual vs fitted values above, we can see that the intial model massively underpredicts for a lot of the items. We think that this is probably due to extreme dichotomy of the number of units sold between the stores. We can see that the store which sold the most units in total tops out at approximately 300,000 units while the stores that sold the least only sold less than 10,000 units. This dichotomy is deadly to a linear model because it ends up fitting close to the middle of the two which in turn means that you simultaneously overpredict and underpredict. Methods of which to help fix this issue will be explored in the next section. 

The thoughts that we expressed in the previous paragraph regarding the dichotomy of the observations is also reflected in the plot of leverages vs cook's distance *Fig.5*. This plot is extremely intersting as it suggests that most of the observations are extremely influential. This again confirms our suspicions about the nature of the data. 

Apart from the heavily categorical variables, all the other variables in the model were isignificant except for dewpoint (at an alpha of .1). This hints that the weather data isn't predictive at all for the number of units sold which is a bit disheartening.

\newpage

## Improvements

```{r CV-linear-mods, cache = TRUE, cache.lazy = FALSE}
set.seed(425)
lminit_mod = train(units ~ . - date - station_nbr - season,
                data = trn_s,
                method = "lm",
                trControl = trainControl(method = "LGOCV", p = 0.8, number = 1),
                metric = "RMSE")

set.seed(425)
lm1_mod = train(units ~ store_nbr + item_nbr + tmax + dewpoint + cool + avgspeed,
                data = trn_s,
                method = "lm",
                trControl = trainControl(method = "LGOCV", p = 0.8, number = 1),
                metric = "RMSE")

```

```{r Tukey-itm-nbr}
tukey_itmnbr = TukeyHSD(aov(units ~ item_nbr, trn_s))
idx_t1 = which(tukey_itmnbr$item_nbr[,4] < 0.05)
t1_sig = tukey_itmnbr$item_nbr[idx_t1,]
t1_grps = strsplit(rownames(t1_sig), "[-]")
t1_grp1 = c()
t1_grp2 = c()
for (i in 1:length(t1_grps)){
  obs_i = t1_grps[[i]]
  t1_grp1[i] = obs_i[1]
  t1_grp2[i] = obs_i[2]
}
t1_grp_df = data.frame("V1" = c(t1_grp1, t1_grp2), "V2" = c(t1_grp2, t1_grp1))

t1_grp_sig = t1_grp_df %>% 
  group_by(V1) %>%
    summarise("diff_from" = n())

diff_item = as.numeric(as.character(t1_grp_sig$V1[t1_grp_sig$diff_from != 10]))

trn_tukey = trn_s

levels(trn_tukey$item_nbr) = c(0, levels(trn_tukey$item_nbr))

trn_tukey$item_nbr[!(trn_tukey$item_nbr %in% diff_item)] = 0

trn_tukey$item_nbr = droplevels(trn_tukey$item_nbr)
```

```{r Tukey-store-nbr}
tukey_strnbr = TukeyHSD(aov(units ~ store_nbr, trn_s))
idx_t2 = which(tukey_strnbr$store_nbr[,4] < 0.05)
t2_sig = tukey_strnbr$store_nbr[idx_t2,]
t2_grps = strsplit(rownames(t2_sig), "[-]")
t2_grp1 = c()
t2_grp2 = c()
for (i in 1:length(t2_grps)){
  obs_i = t2_grps[[i]]
  t2_grp1[i] = obs_i[1]
  t2_grp2[i] = obs_i[2]
}

t2_grp_df = data.frame("V1" = c(t2_grp1, t2_grp2), "V2" = c(t2_grp2, t2_grp1))

t2_grp_sig = t2_grp_df %>% 
  group_by(V1) %>%
    summarise("diff_from" = n())

diff_store = as.numeric(as.character(t2_grp_sig$V1[t2_grp_sig$diff_from >= 10]))

levels(trn_tukey$store_nbr) = c(0, levels(trn_tukey$store_nbr))

trn_tukey$store_nbr[!(trn_tukey$store_nbr %in% diff_store)] = 0

trn_tukey$store_nbr = droplevels(trn_tukey$store_nbr)
```

```{r modify-tst-tukey}
tst_tukey = tst

levels(tst_tukey$item_nbr) = c(0, levels(tst_tukey$item_nbr))
tst_tukey$item_nbr[!(tst_tukey$item_nbr %in% diff_item)] = 0
tst_tukey$item_nbr = droplevels(tst_tukey$item_nbr)


levels(tst_tukey$store_nbr) = c(0, levels(tst_tukey$store_nbr))
tst_tukey$store_nbr[!(tst_tukey$store_nbr %in% diff_store)] = 0
tst_tukey$store_nbr = droplevels(tst_tukey$store_nbr)
```

```{r interactions-tukey, cache = TRUE, cache.lazy = FALSE}
set.seed(425)
lm2_mod = train(units ~ store_nbr * item_nbr + tmax + dewpoint + cool + avgspeed,
                data = trn_tukey,
                method = "lm",
                trControl = trainControl(method = "LGOCV", p = 0.8, number = 1),
                metric = "RMSE")
```

```{r polynomial-initial}
set.seed(425)
lmp1_mod = lm(units ~ tmax, data = trn_s )
lmp2_mod = lm(units ~ dewpoint, data = trn_s )
lmp3_mod = lm(units ~ cool, data = trn_s )
```

```{r first-poly}
set.seed(425)
lm3_mod = train(units ~ store_nbr + item_nbr + poly(tmax, 2) + dewpoint + cool + avgspeed,
                data = trn_s,
                method = "lm",
                trControl = trainControl(method = "LGOCV", p = 0.8, number = 1),
                metric = "RMSE")
```


Before we began exploring these models further, we chose to validate the results by creating a validation split of 20% of the training data. We chose to use an 80-20 split of the data because it's usually around that by convention. The reason why this split is necessary is that it allows us to test our model on data that it hasn't seen yet, something that we are graded on with the testing split given to use by Walmart. By creating our own data split, we managed to avoid overloading the kaggle submissions and having to spend minutes waiting for results.

### Original Data Set

When looking at the summary of the first created model, we could see that all of the weather data was insignificant. To further verify this suspicion, we decided to start the improvements section by regressing the number of units sold on the store number alone. The reason why we used this as a starting point is because from the EDA, it became very clear that the stores differed extremely drastically from each other in terms of units sold. While this "initial" model performed much worse (which was expected) as we removed all the other variables besides store number. A lot of the stores were shown to be significant which means that the stores are significantly different from eachother. From this we interpreted the results to mean that we should focus on incorporating the store number and the item numbers into all of our models.

Next, we decided to try and clean up the weather data a bit. From the plot of all the correlations shown in the EDA, we can see that the weather aspect of the data were all highly correlated with eachother. While it's important to note that high multicollinearity doesn't decrease the predictive power of the model, multicollinearity could lead to overfitting of the model. The best way to explore this is through the variance inflation factors. The idea behind variance inflation factors is that if you regress a term on the rest of the independent variables, you can see which % of variation in that variable is explained with from the rest of the variables. This allows you to keep track of multicollinearity. While normally, I would iteratively remove the highest VIF, in this case, it's extremely clear from the EDA that all of the weather variables are very strongly correlated with eachother. Therefore, we felt comfortable removing multiple variables at one time. In *Figure.5* we can see that the VIF's are way over the "cutoff" of 5. To remedy this, we removed wetbulb, heat, and the minimum temperature. We also noted that we initially missed that there were two highly collinear wind speed terms, average wind speed and results wind speed. We decided to remove resultspeed because avgerage windspeed was far more intuitive to us. This model actually performed better than the initial model. However it must be noted that the difference was almost negligible as the RMSE improved by `r round(lminit_mod$results$RMSE - lm1_mod$results$RMSE, 5)`. This was expected as the models in this "newer" model and the original model all covered extremely similar information. All that we managed to achieve by removing highly correlated variables was improve the interpretability of the model.

The next thing that we wanted to explore were interaction terms. The very first term we wanted to explore was between item and store number. The reason why we thought that this would be relevant was because it would allow the model to account for the relationship between store and item which we could see in *Figure.7* and *Figure.8*. This relationship should theoretically be extremely relevant for some combinations as we can see that the vast majority of combinations never sold any units. The idea behind interaction terms is that it allows you to model more than just a linear relationship between the dependent and independent variable. The reason why we were so optimistic about the relationship between store numberand item number is that the iteraction between two categorical variables is an and between the two variables. Since so many of our store-item combinations can be reduced to 0, we'd hope that the interaction term will just negate the intercept and thus reduce the prediction to 0.

However, while the idea outlined above sounded relatively promising, there are very clear issues when trying to fit a model of this size as just using this interaction would create a model with `r 45 * 111 + 2` variables. To try and work around this, we decided that we wanted to explore possibly reducing the number of factors that are present in the item numbers and store numbers. In order to do this, we conducted Tukey's HSD test on items and store number. 

### Partially Manipulated Data Set

Since we had many levels for each of these categorical variables, 111 for items and 45 stores, we decided that Tukey's Honest Significant Difference was the best method to apply when trying to distinguish between levels. The insight behind Tukey is that 

When we conducted Tukey's test on each of the two variables. When it came to item numbers, many of the items were insignificant from eachother, this isn't really suprising because in the EDA, we found out that there were many items which barely any units at all per day across each store. This was also reflected in our analysis with Tukey as the number of unique items was reduced to `r length(diff_item) + 1`. We used 0 to represent all of the items which were determined to "not" be different. We conducted the exact same analysis on store numbers. However, it turns out that a far larger percent of stores differed from eachother which left us with `r length(diff_store) + 1`. We then tried fitting a model with interactions between item number and store number, however, we were sorely dissapointed as the model performed worse in both our validation subset and the kaggle grader and ended up with a kaggle score of *.748*.

### Fully Manipulated Data Set

Instead of relying on the interaction terms between item number and store number to zero out predictions for certain combos. We decided to just remove those combinations from the data set and just predict 0 for them. This would help in many different ways as it lowers the size of the data set to approximately 200,000 observations. 

```{r box-cox-findLambda}
bc_trn = trn_s
bc_trn$units_bc = (bc_trn$units + 1)

lm1_bc = lm(units_bc ~ . - date - month - units - station_nbr,
                data = bc_trn)

bc_lm1 = boxcox(lm1_bc, lambda = seq(-20,-10, .1))

lambda = bc_lm1$x[which.max(bc_lm1$y)]
```

```{r box-cox-transformation}
# transform our y according to box cox lambda
trn_bc_trans = bc_trn
trn_bc_trans$units_bc = (trn_bc_trans$units_bc^lambda - 1)/lambda

lm1_bctrans = lm(units_bc ~ . - date - month - units - station_nbr,
                data = trn_bc_trans)

# residuals are a lot better, though still not good.  
# qqnorm(lm1_bctrans$residuals)
# qqline(lm1_bctrans$residuals)
```

```{r without-bc-transformation}
# qqnorm(lm1_bc$residuals)
# qqline(lm1_bc$residuals)
```


```{r, eval = FALSE}
ss = read_csv("data/sampleSubmission.csv")
f = predict(lm2_mod, tst_tukey)
f = round(f, 0)
f[f < 0] = 0
ss$units = f
write_csv(ss, "data/first_trial.csv")
```


#### Extra models

```{r trnsell-poisson-reg, eval = FALSE}
# pois reg p shitty
set.seed(425)
poiss_mod = train(units ~ store_nbr + item_nbr + resultspeed + heat,
                data = trn_sell,
                method = "glmnet",
                family = "poisson",
                trControl = trainControl(method = "LGOCV", p = 0.8, number = 1),
                metric = "RMSE"
                )
```

```{r EXTR-randomForest, eval = FALSE}
library(doParallel)
c1 = makeForkCluster(3)
registerDoParallel(c1)

rf_mod = train(units ~ . - station_nbr - date,
               data = trn_tukey,
               method = "rf",
               trControl = trainControl(method = "cv",
                                        number = 5),
               metric = "RMSE",
               verbose = FALSE
               )

stopCluster(c1)
```


## Appendix

*Fig.1* Initial state of weather data

```{r, warning = FALSE}
weather_data = read_csv("data/weather.csv")

w_g = weather_data %>%
    group_by(station_nbr) %>%
    group_split()

miss_vars = lapply(w_g, function(dat){
    miss_vals = c()
    for(i in 3:ncol(dat)){
        miss_vals[i-2] = ifelse(sum(is.na(dat[,i])) > 0, paste(sum(is.na(dat[,i])), "NA values"), 
                                ifelse(sum(dat[,i] == "M") > 0, paste(sum(dat[,i] == "M"), "M"), 
                                    ifelse(sum(dat[,i] == "-") > 0, paste(sum(dat[,i] == "-"),"VNA"),
                                           ifelse(sum(dat[,i] == "T"), paste(sum(dat[,i] == "T"), "Trace"), "OK"))))
    }
    names(miss_vals) <- names(dat)[3:ncol(dat)]
    return(miss_vals)
})

weather_status = as.data.frame(do.call(rbind, miss_vars))
```


```{r AP-init-weather, warning = FALSE}
weather_status %>%
    kable(caption = "M, NA, and VNA represent the count of M's, -'s, and NA's respectively") %>%
    kable_styling("striped", latex_options = c("scale_down", "hold_position"))
```

*Fig.2* Sample of plot of residuals of first model

```{r AP-pred-v-res-1}
sam = sample(1:nrow(trn_s), round(nrow(trn_s) * .4, 0))
plot(predict(md1)[sam], md1$residuals[sam], xlab = "Fitted", ylab = "Residuals")
```

*Fig.3* Distributions of sales by store

```{r APP-dist-sale}
c19
```

*Fig.4* Plot of first linear model

```{r APP-resid-lev-p1, cache = TRUE}
plot(md1, which = 5)
```

*Fig.5* Initial VIFs

```{r initial-VIF}
vif(lm(units ~ tmax + tmin + dewpoint + wetbulb + heat + cool, data = trn_s))
```

*Fig.6* Reduced VIFs

```{r reduced-VIF}
vif(lm(units ~ tmax + dewpoint + cool, data = trn_s))
```

*Fig.7* Distribution of Units Sold by Store and Item

```{r dist-un-str}
trn_s %>% 
    group_by(store_nbr, item_nbr) %>% 
    summarise("totUnits" = sum(units)) %>%
    ggplot() +
    geom_histogram(aes(x = totUnits)) + 
    labs(x = "Total Units Sold")
```

*Fig.8* Distribution of Units Sold by Store and Item without 0

```{r dist-un-str-no0}
trn_s %>% 
    group_by(store_nbr, item_nbr) %>% 
    summarise("totUnits" = sum(units)) %>%
    filter(!(totUnits == 0)) %>%
    ggplot() +
    geom_histogram(aes(x = totUnits)) + 
    labs(x = "Total Units Sold")
```

*Fig.9* Residuals of Units vs fitted

```{r fig9}
plot(trn_s$tmax, lmp1_mod$residuals, xlab = "Max Temperature", ylab = "Residuals")
```

*Fig.10* Residuals of Units vs fitted

```{r fig9}
plot(trn_s$tmax, lmp1_mod$residuals, xlab = "Max Temperature", ylab = "Residuals")
```

*Fig.11* Residuals of Units vs fitted

```{r fig9}
plot(trn_s$tmax, lmp3_mod$residuals, xlab = "Max Temperature", ylab = "Residuals")
```

#### Data Dictionary

- `item_nbr`: categorical variable for each of the 111 types of store items
- `nbr_unit`: integer value of how many units of the corresponding item were sold
- `tmax`: maximum temperature (degrees Fahrenheit)
- `tmin`: minimum temperature (degrees Fahrenheit)
- `dewpoint`: average dew point; the higher the dew point, the greater the amount of moisture in the air
- `wetbulb`: average wet bulb temperature; lowest temperature to which air can be cooled by the evaporation of water into the air at constant pressure
- `heat`: heating degree day (HDD) with season beginning with July; measures the demand for energy needed to heat a building; number of degrees that a day's average temperature is below 65ºF
- `cool`: cooling degree day (CDD) with season beginning with January; measures the demand for energy needed to cool buildings; number of degrees that a day's average temperature is above 65ºF
- `codesum`: binary: 1 if a weather event occurred, else 0.
- `stnpressure`: average station pressure
- `resultspeed`: resultant wind speed; average of all wind speeds at a given place for a certain period
- `resultdir`: resultant direction; average of all wind directions at a given place for a certain period
- `avgspeed`: average wind speed

\newpage

#### EDA


```{r, include = TRUE, cache = TRUE}
gridExtra::grid.arrange(c04, c05, c06, 
                        c07, c08, c09, 
                        c10, c11, c12,
                        c13, c14, c15, ncol = 3)
```

\newpage

## References

[^1]: [Morning Routine: What's the First Thing You Do When You Wake Up?](https://www.apartmenttherapy.com/morning-routine-habits-wake-up-196378)

[^2]: [Retail and Supply Chain](https://www.aer.com/industry/supply-chain/)

[^3]: [Kaggle: Walmart Recruiting II: Sales in Stormy WEather](https://www.kaggle.com/c/walmart-recruiting-sales-in-stormy-weather/data)

[^4]: [Weather Prediction Accuracy](https://scijinks.gov/forecast-reliability/)
